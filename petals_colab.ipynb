{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/petals-colab/blob/main/petals_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrgcDwZxgDOe"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/bigscience-workshop/petals\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from petals import AutoDistributedModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"enoch/llama-65b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, add_bos_token=False)\n",
        "\n",
        "model = AutoDistributedModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer('A cat in French is \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake_token = tokenizer(\"^\")[\"input_ids\"][0]  # Workaround to make SentencePiece .decode() keep leading spaces\n",
        "\n",
        "with model.inference_session(max_length=512) as sess:\n",
        "    while True:\n",
        "        prompt = input('Human: ')\n",
        "        if prompt == \"\":\n",
        "            break\n",
        "        prefix = f\"Human: {prompt}\\nFriendly AI:\"\n",
        "        prefix = tokenizer(prefix, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "        print(\"Friendly AI:\", end=\"\", flush=True)\n",
        "\n",
        "        while True:\n",
        "            outputs = model.generate(\n",
        "                prefix, max_new_tokens=1, do_sample=True, top_p=0.9, temperature=0.75, session=sess\n",
        "            )\n",
        "            outputs = tokenizer.decode([fake_token, outputs[0, -1].item()])[1:]\n",
        "            print(outputs, end=\"\", flush=True)\n",
        "            if \"\\n\" in outputs:\n",
        "                break\n",
        "            prefix = None  # Prefix is passed only for the 1st token of the bot's response"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
